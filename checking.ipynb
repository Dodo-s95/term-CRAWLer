{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "import spacy\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "## Named entities extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# modify tokenizer infix patterns for avoiding hyphenated words being split (several terms are hyphenated)\n",
    "\n",
    "infixes = (\n",
    "    LIST_ELLIPSES\n",
    "    + LIST_ICONS\n",
    "    + [\n",
    "        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
    "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "        ),\n",
    "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
    "    ]\n",
    ")\n",
    "\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp.tokenizer.infix_finditer = infix_re.finditer\n",
    "\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ners = set()\n",
    "\n",
    "for txt in tqdm(glob.iglob(\"../corpus_txt/*.txt\")):\n",
    "    with open(txt, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = f.readlines()[0]\n",
    "        doc = nlp(txt)\n",
    "        local_ners = doc.ents\n",
    "        str_ners = [el.text.lower() for el in local_ners]\n",
    "        ners.update(set(str_ners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 20"
   ]
  },
  {
   "source": [
    "## Checking process\n",
    "\n",
    "Give the whole corpus to spacy -> Extract NER \n",
    "\n",
    "For each term extraction:\n",
    "- filter the terms (remove terms which are NER AND the ones which are variations if primary term) using list of NER\n",
    "- Remove the duplicates\n",
    "- random sample 200 and check (measure precision and recall)\n",
    "\n",
    "\n",
    "1st extraction : Basic stuff of termsuite "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Extractions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1st extraction : Basic "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_1 = pd.read_csv('../extractions/1.tsv', sep='\\t')\n",
    "extract_1.key = extract_1.key.apply(lambda x: x.split(':')[1][1:])\n",
    "extract_1.drop_duplicates(subset=\"key\", keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_ner_extract_1 = extract_1[~extract_1[\"key\"].isin(ners)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a placeholder for evaluation\n",
    "# (FALSE means un-thicked box in excel)\n",
    "not_ner_extract_1['evaluation'] = 'FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_ner_extract_1.sample(400, random_state = seed)[['key','evaluation']].to_csv('../evaluation/extract_1.csv', index=False, header=['candidate','evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}